{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "INFO:tensorflow:Assets written to: ./models/simple_preprocessor/assets\n",
      "norm_features: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing recipe\n",
    "\n",
    "class SimplePreprocessor(tf.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.norm = tf.keras.layers.Normalization()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.norm.adapt(data)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float32)])\n",
    "    def train_fn(self, examples):\n",
    "        return {\n",
    "            \"normalized_features\": self.norm(examples)\n",
    "        }\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float32)])\n",
    "    def serving_fn(self, examples):\n",
    "        return {\n",
    "            \"normalized_features\": self.norm(examples)\n",
    "        }\n",
    "\n",
    "\n",
    "p = SimplePreprocessor()\n",
    "ds = tf.data.Dataset.range(100).batch(5, drop_remainder=True).map(lambda x: tf.cast(tf.reshape(x, [-1, 1]), tf.float32))\n",
    "for x in ds.take(1):\n",
    "    print(x.shape)\n",
    "p.fit(ds)\n",
    "\n",
    "tf.saved_model.save(p, \"./models/simple_preprocessor\", signatures={\"serving_default\": p.serving_fn, \"train_default\": p.train_fn})\n",
    "loaded = tf.saved_model.load(\"./models/simple_preprocessor\")\n",
    "loaded.signatures[\"serving_default\"]\n",
    "norm_features = loaded.signatures[\"serving_default\"](tf.constant([[49.5]]))[\"normalized_features\"].numpy()[0][0]\n",
    "print(f\"norm_features: {norm_features:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_pipeline as ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low level implementation with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amount': TensorShape([16]), 'Class': TensorShape([16]), 'V1': TensorShape([16]), 'V10': TensorShape([16]), 'V11': TensorShape([16]), 'V12': TensorShape([16]), 'V13': TensorShape([16]), 'V14': TensorShape([16]), 'V15': TensorShape([16]), 'V16': TensorShape([16]), 'V17': TensorShape([16]), 'V18': TensorShape([16]), 'V19': TensorShape([16]), 'V2': TensorShape([16]), 'V20': TensorShape([16]), 'V21': TensorShape([16]), 'V22': TensorShape([16]), 'V23': TensorShape([16]), 'V24': TensorShape([16]), 'V25': TensorShape([16]), 'V26': TensorShape([16]), 'V27': TensorShape([16]), 'V28': TensorShape([16]), 'V3': TensorShape([16]), 'V4': TensorShape([16]), 'V5': TensorShape([16]), 'V6': TensorShape([16]), 'V7': TensorShape([16]), 'V8': TensorShape([16]), 'V9': TensorShape([16])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = ip.Preprocessor()\n",
    "ds = tf.data.TFRecordDataset([\"data/example_gen/test.tfrecord\"])\n",
    "\n",
    "parsed_ds = ds.map(lambda x: tf.io.parse_single_example(x, ip.TRAIN_SCHEMA)).batch(16)\n",
    "for x in parsed_ds.take(1):\n",
    "    print({k: v.shape for k, v in x.items()})\n",
    "p.fit(parsed_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 29)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in parsed_ds.map(p.preprocessing_fn).take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage build-in dataset capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 29)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "src = \"data/example_gen/train.tfrecord\"\n",
    "ds = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=src,\n",
    "        batch_size=64,\n",
    "        features=ip.TRAIN_SCHEMA,\n",
    "        label_key=ip.LABEL_KEY,\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle_buffer_size=10000,\n",
    "        shuffle_seed=42,\n",
    "        num_epochs=1,\n",
    "        prefetch_buffer_size=1000,\n",
    "        reader_num_threads=8,\n",
    "        parser_num_threads=8,\n",
    "        drop_final_batch=True,\n",
    "    )\n",
    "\n",
    "# fit the preprocessor\n",
    "p.fit(ds.map(lambda x, _: x)) \n",
    "\n",
    "for x, y in ds.map(lambda x, y: (p.serving_fn(x), tf.reshape(y, (-1, 1)))).take(1).as_numpy_iterator():\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    # print({k: v.shape for k, v in x.items()})\n",
    "    # print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get datasets and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ip.Preprocessor()\n",
    "assert not p.norm.is_adapted\n",
    "train_ds, val_ds = ip.get_datasets(\n",
    "    preprocessor=p,\n",
    "    train_src=\"data/example_gen/train.tfrecord\",\n",
    "    val_src=\"data/example_gen/test.tfrecord\",\n",
    ")\n",
    "\n",
    "# validate the preprocessor is adapted to the training data\n",
    "assert p.norm.is_adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 29)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "for x, y  in train_ds:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building with flax, clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.training import train_state\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import orbax\n",
    "import clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardFraudModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "def init_model(rng, input_shape):\n",
    "    model = CreditCardFraudModel()\n",
    "    params = model.init(rng, jnp.ones(input_shape, jnp.float32))\n",
    "    return model, params\n",
    "\n",
    "def create_train_state(rng, input_shape, learning_rate=1e-3):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    model, params = init_model(rng, input_shape)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx\n",
    "    )\n",
    "\n",
    "batch_size = 64\n",
    "n_features = 29\n",
    "input_shape = (batch_size, n_features)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = create_train_state(rng, input_shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 29)\n",
      "(64, 1)\n",
      "[[ 1.5086735  -1.2670166  -1.2634001  ... -0.0185016  -0.06934007\n",
      "   1.3445339 ]\n",
      " [ 2.7657025  -0.8945842   0.03694475 ...  0.16129413  0.11783955\n",
      "  -0.9416372 ]\n",
      " [-3.7643168  -3.4835062  -0.24297953 ... -0.5262222   0.3545389\n",
      "   1.5926738 ]\n",
      " ...\n",
      " [ 3.9149642   0.85186887  0.39583114 ... -0.5226201   0.03370051\n",
      "  -0.59020585]\n",
      " [ 1.7115073   0.9279888  -0.43129742 ... -0.14537239 -0.29199368\n",
      "   1.0793302 ]\n",
      " [ 0.65398043  0.8988843  -0.40111452 ... -0.45063296 -0.16948257\n",
      "   0.9866318 ]]\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = ip.get_datasets(\n",
    "    preprocessor=p,\n",
    "    train_src=\"data/example_gen/train.tfrecord\",\n",
    "    val_src=\"data/example_gen/test.tfrecord\",\n",
    ")\n",
    "for x, y in train_ds:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    # check if y is all 0s\n",
    "    if jnp.all(y == 0):\n",
    "        continue\n",
    "\n",
    "    print(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import metrics\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class Precision(metrics.Metric):\n",
    "  \"\"\"Computes the precision from model outputs `logits` and `labels`.\"\"\"\n",
    "\n",
    "  true_positives: jnp.array\n",
    "  pred_positives: jnp.array\n",
    "\n",
    "  @classmethod\n",
    "  def from_model_output(cls, *, logits: jnp.array, labels: jnp.array,\n",
    "                        **_) -> metrics.Metric:\n",
    "    assert logits.shape[-1] == 2, \"Expected binary logits.\"\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return cls(\n",
    "        true_positives=((preds == 1) & (labels == 1)).sum(),\n",
    "        pred_positives=(preds == 1).sum(),\n",
    "    )\n",
    "\n",
    "  def merge(self, other: metrics.Metric) -> metrics.Metric:\n",
    "    # Note that for precision we cannot average metric values because the\n",
    "    # denominator of the metric value is pred_positives and not every batch of\n",
    "    # examples has the same number of pred_positives (as opposed to e.g.\n",
    "    # accuracy where every batch has the same number of)\n",
    "    return type(self)(\n",
    "        true_positives=self.true_positives + other.true_positives,\n",
    "        pred_positives=self.pred_positives + other.pred_positives,\n",
    "    )\n",
    "\n",
    "  def compute(self):\n",
    "    return self.true_positives / self.pred_positives\n",
    "\n",
    "  def empty(self):\n",
    "    return type(self)(true_positives=0, pred_positives=0)\n",
    "\n",
    "  @classmethod\n",
    "  def empty(cls) -> \"Precision\":\n",
    "    return cls(true_positives=0, pred_positives=0)\n",
    "\n",
    "@flax.struct.dataclass  # <-- required for JAX transformations\n",
    "class MetricCollection(metrics.Collection):\n",
    "  loss : metrics.Average.from_output('loss')\n",
    "  accuracy : metrics.Accuracy\n",
    "  precision: Precision\n",
    "\n",
    "metric_collection = MetricCollection.empty()\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: train_state.TrainState, x, y, train_metrics):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn(\n",
    "        params,\n",
    "      x=x)\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, y)\n",
    "    loss = jnp.mean(loss)\n",
    "    return loss, logits\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  logits = jnp.concatenate([1 - logits, logits], axis=-1)\n",
    "\n",
    "  return state, train_metrics.merge(MetricCollection.single_from_model_output(\n",
    "    loss=loss,\n",
    "    labels=y.squeeze(),\n",
    "    logits=logits,\n",
    "))\n",
    "  \n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state: train_state.TrainState, x, y, eval_metrics):\n",
    "  \"\"\"Evaluates `state` on `x` and `y`.\"\"\"\n",
    "  logits = state.apply_fn(\n",
    "      state.params,\n",
    "      x=x)\n",
    "  loss = optax.sigmoid_binary_cross_entropy(logits, y)\n",
    "  loss = jnp.mean(loss)\n",
    "\n",
    "  logits = jnp.concatenate([1 - logits, logits], axis=-1)\n",
    "\n",
    "  \n",
    "\n",
    "  return eval_metrics.merge(MetricCollection.single_from_model_output(\n",
    "    loss=loss,\n",
    "    labels=y.squeeze(),\n",
    "    logits=logits,\n",
    "))\n",
    "  \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import periodic_actions\n",
    "\n",
    "class TensorboardCallback(periodic_actions.PeriodicCallback):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def write_metrics(step: int, t:float, *, writer, train_metrics, eval_metrics):\n",
    "        writer.write_scalars(step, {f\"train/{k}\": v for k, v in train_metrics.compute().items()})\n",
    "        writer.write_scalars(step, {f\"eval/{k}\": v for k, v in eval_metrics.compute().items()})\n",
    "        \n",
    "class ReportProgress(periodic_actions.ReportProgress):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, step, t = None, **kwargs):\n",
    "        return super().__call__(step, t)\n",
    "    \n",
    "    def _apply(self, step, t, **kwargs):\n",
    "        super()._apply(step, t)\n",
    "\n",
    "class Profile(periodic_actions.Profile):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, step, t = None, **kwargs):\n",
    "        return super().__call__(step, t)\n",
    "    \n",
    "    def _apply(self, step, t, **kwargs):\n",
    "        super()._apply(step, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9999\n",
      "{'loss': Array(1.07910274e-07, dtype=float32), 'accuracy': Array(1., dtype=float32), 'precision': Array(1., dtype=float32)} {'loss': Array(1.0784902e-07, dtype=float32), 'accuracy': Array(1., dtype=float32), 'precision': Array(1., dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from clu import metric_writers\n",
    "from clu import periodic_actions\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.WARNING)\n",
    "# create random 1 and 0 labels with shape (batch_size, 1)\n",
    "batch_x = jax.random.normal(rng, shape=(batch_size, n_features))\n",
    "batch_y = jax.random.randint(rng, shape=(batch_size, 1), minval=0, maxval=2)\n",
    "\n",
    "\n",
    "logdir = \"./logs\"\n",
    "n_epochs = 10\n",
    "n_batches_per_epoch = 1000\n",
    "total_steps = n_epochs * n_batches_per_epoch\n",
    "writer = metric_writers.create_default_writer(logdir)\n",
    "\n",
    "\n",
    "hooks = [\n",
    "    # Outputs progress via metric writer (in this case logs & TensorBoard).\n",
    "    ReportProgress(\n",
    "        num_train_steps=total_steps,\n",
    "        every_steps=n_batches_per_epoch, writer=writer),\n",
    "    Profile(logdir=logdir),\n",
    "    TensorboardCallback(callback_fn=TensorboardCallback.write_metrics, every_steps=n_batches_per_epoch),\n",
    "]\n",
    "\n",
    "state = create_train_state(rng, input_shape)\n",
    "train_metrics = metric_collection.empty()\n",
    "eval_metrics = metric_collection.empty()\n",
    "\n",
    "\n",
    "for step in range(total_steps):\n",
    "    state, train_metrics = train_step(state, batch_x, batch_y, train_metrics)\n",
    "    eval_metrics = eval_step(state, batch_x, batch_y, eval_metrics)\n",
    "    for hook in hooks:\n",
    "        hook(step, writer=writer, train_metrics=train_metrics, eval_metrics=eval_metrics)\n",
    "    \n",
    "    if not step % n_batches_per_epoch:\n",
    "        train_metrics = metric_collection.empty()\n",
    "        eval_metrics = metric_collection.empty()\n",
    "        \n",
    "# for epoch in range(10):\n",
    "#     train_metrics = metric_collection.empty()\n",
    "#     eval_metrics = metric_collection.empty()\n",
    "#     for b_update in range(1000):\n",
    "#         state, train_metrics = train_step(state, batch_x, batch_y, train_metrics)\n",
    "#         eval_metrics = eval_step(state, batch_x, batch_y, eval_metrics)\n",
    "        # show wrong predictions\n",
    "print(f\"step {step}\")\n",
    "print(train_metrics.compute(), eval_metrics.compute())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5cf813b52b07323\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5cf813b52b07323\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path absolute path to \"./models\"\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./models/checkpoints/\")\n",
    "model_dir = path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "from flax.training.train_state import TrainState\n",
    "from orbax.export import ExportManager, JaxModule, ServingConfig\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def create_manager(model_dir):\n",
    "    options = ocp.CheckpointManagerOptions(\n",
    "        max_to_keep=3,\n",
    "        save_interval_steps=2,\n",
    "        create=True\n",
    "    )\n",
    "\n",
    "    mngr = ocp.CheckpointManager(\n",
    "        model_dir,\n",
    "        ocp.PyTreeCheckpointer(),\n",
    "        options=options\n",
    "    )\n",
    "    return mngr\n",
    "\n",
    "def restore_or_create_state(mngr, rng, input_shape, reinit=False):\n",
    "    if mngr.latest_step() is None or reinit:\n",
    "        return create_train_state(rng, input_shape)\n",
    "    target = {\n",
    "        \"model\": create_train_state(rng, input_shape)\n",
    "    }\n",
    "    restored_state = mngr.restore(mngr.latest_step(), items=target)[\"model\"]\n",
    "    return restored_state\n",
    "   \n",
    "\n",
    "def to_saved_model(state, preprocessing_fn, output_dir, etr=None, model_name=\"creditcard\"):\n",
    "    # Construct a JaxModule where JAX->TF conversion happens.\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    jax_module = JaxModule(\n",
    "        state.params, \n",
    "        state.apply_fn, \n",
    "        trainable=False,\n",
    "        jit_compile=False,\n",
    "        jax2tf_kwargs={\n",
    "            \"enable_xla\": False\n",
    "        },\n",
    "        input_polymorphic_shape='(b, ...)')\n",
    "    # Export the JaxModule along with one or more serving configs.\n",
    "    export_mgr = ExportManager(\n",
    "        jax_module,\n",
    "        [\n",
    "            ServingConfig(\n",
    "                \"serving_default\",\n",
    "                tf_preprocessor=preprocessing_fn,\n",
    "                # tf_postprocessor=exampe1_postprocess\n",
    "                extra_trackable_resources=etr,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    export_mgr.save(os.path.join(output_dir, model_name, timestamp))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/orbax/checkpoint/type_handlers.py:1346: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs.\n",
      "  warnings.warn(\n",
      "ERROR:absl:Could not start profiling: Profile has already been started. Only one profile may be run at a time.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/clu/periodic_actions.py\", line 359, in _start_session\n",
      "    profiler.start(logdir=self._logdir)\n",
      "  File \"/Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/clu/profiler.py\", line 38, in start\n",
      "    jax.profiler.start_trace(logdir)\n",
      "  File \"/Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/venv/lib/python3.9/site-packages/jax/_src/profiler.py\", line 118, in start_trace\n",
      "    raise RuntimeError(\"Profile has already been started. \"\n",
      "RuntimeError: Profile has already been started. Only one profile may be run at a time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': Array(0.0021319, dtype=float32), 'accuracy': Array(0.99951565, dtype=float32), 'precision': Array(0.9056604, dtype=float32)} {'loss': Array(0.00121001, dtype=float32), 'accuracy': Array(0.99953127, dtype=float32), 'precision': Array(1., dtype=float32)}\n",
      "{'loss': Array(0.00489452, dtype=float32), 'accuracy': Array(0.99921876, dtype=float32), 'precision': Array(1., dtype=float32)}\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/creditcard/20231220133420/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/creditcard/20231220133420/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = ip.Preprocessor()\n",
    "train_ds, val_ds = ip.get_datasets(\n",
    "    preprocessor=p,\n",
    "    train_src=\"data/example_gen/train.tfrecord\",\n",
    "    val_src=\"data/example_gen/test.tfrecord\",\n",
    ")\n",
    "\n",
    "writer = metric_writers.create_default_writer(logdir)\n",
    "hooks = [\n",
    "    # Outputs progress via metric writer (in this case logs & TensorBoard).\n",
    "    ReportProgress(\n",
    "        num_train_steps=total_steps,\n",
    "        every_steps=n_batches_per_epoch, writer=writer),\n",
    "    Profile(logdir=logdir),\n",
    "    TensorboardCallback(callback_fn=TensorboardCallback.write_metrics, every_steps=n_batches_per_epoch),\n",
    "]\n",
    "\n",
    "n_train_steps = 1000\n",
    "n_eval_staps = 100\n",
    "\n",
    "mngr = create_manager(model_dir)\n",
    "state = restore_or_create_state(mngr, rng, input_shape)\n",
    "saved_model_dir = \"./models/saved_model\"\n",
    "\n",
    "n_steps_taken = 0\n",
    "for epoch in range(10):\n",
    "    train_metrics = metric_collection.empty()\n",
    "    eval_metrics = metric_collection.empty()\n",
    "    for step in range(n_train_steps):\n",
    "        x, y = next(train_ds)\n",
    "        state, train_metrics = train_step(state, x, y, train_metrics)\n",
    "\n",
    "    for step in range(n_eval_staps):\n",
    "        x, y = next(val_ds)\n",
    "        eval_metrics = eval_step(state, x, y, eval_metrics)\n",
    "        for hook in hooks:\n",
    "            hook(n_steps_taken, writer=writer, train_metrics=train_metrics, eval_metrics=eval_metrics)\n",
    "\n",
    "        mngr.save(step, {\"model\": state})\n",
    "        n_steps_taken += 1\n",
    "\n",
    "\n",
    "print(\n",
    "    train_metrics.compute(),\n",
    "    eval_metrics.compute(), \n",
    ")\n",
    "restored_state = restore_or_create_state(mngr, rng, input_shape)\n",
    "\n",
    "eval_m = metric_collection.empty()\n",
    "for _ in range(100):\n",
    "    x, y = next(val_ds)\n",
    "    eval_m = eval_step(restored_state, x, y, eval_m)\n",
    "\n",
    "print(eval_m.compute())\n",
    "\n",
    "to_saved_model(restored_state, p.serving_fn, saved_model_dir, etr={\"preprocessor\": p.norm})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = tf.data.TFRecordDataset([\"data/example_gen/test.tfrecord\"])\n",
    "parsed_ds = raw_ds.map(lambda x: tf.io.parse_single_example(x, ip.SERVING_SCHEMA)).batch(16)\n",
    "batch = next(iter(parsed_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/saved_model'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last credit card model in saved_model_dir/models\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "creditcard_model_dir = os.path.join(saved_model_dir, \"creditcard\")\n",
    "versions = [int(v) for v in os.listdir(creditcard_model_dir)]\n",
    "latest_version = max(versions)\n",
    "latest_model = os.path.join(creditcard_model_dir, str(latest_version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -9.401173 ]\n",
      " [ -8.420352 ]\n",
      " [ -8.5709095]\n",
      " [ -9.2276325]\n",
      " [ -7.2797523]\n",
      " [ -8.968629 ]\n",
      " [ -9.036577 ]\n",
      " [ -8.611546 ]\n",
      " [ -8.864519 ]\n",
      " [ -6.95437  ]\n",
      " [ -7.7919803]\n",
      " [ -9.988268 ]\n",
      " [ -9.603817 ]\n",
      " [-11.05954  ]\n",
      " [-11.539463 ]\n",
      " [-10.359598 ]], shape=(16, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = tf.saved_model.load(latest_model).signatures[\"serving_default\"](**batch)[\"output_0\"]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50.0],\n",
       " [14.949999809265137],\n",
       " [7.699999809265137],\n",
       " [6.989999771118164],\n",
       " [460.7099914550781],\n",
       " [68.0],\n",
       " [56.310001373291016],\n",
       " [30.520000457763672],\n",
       " [19.989999771118164],\n",
       " [40.22999954223633],\n",
       " [10.0],\n",
       " [9.989999771118164],\n",
       " [505.92999267578125],\n",
       " [33.369998931884766],\n",
       " [126.0],\n",
       " [385.9800109863281]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"Amount\"].numpy().reshape(-1, 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Amount': [[50.0],\n",
       "   [14.949999809265137],\n",
       "   [7.699999809265137],\n",
       "   [6.989999771118164],\n",
       "   [460.7099914550781],\n",
       "   [68.0],\n",
       "   [56.310001373291016],\n",
       "   [30.520000457763672],\n",
       "   [19.989999771118164],\n",
       "   [40.22999954223633],\n",
       "   [10.0],\n",
       "   [9.989999771118164],\n",
       "   [505.92999267578125],\n",
       "   [33.369998931884766],\n",
       "   [126.0],\n",
       "   [385.9800109863281]]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_batch = []\n",
    "parsed_ds = raw_ds.map(lambda x: tf.io.parse_single_example(x, ip.SERVING_SCHEMA)).batch(16)\n",
    "serving_batch = [{\"Amount\": x[\"Amount\"].numpy().reshape(-1, 1).tolist()} for x in parsed_ds.take(1)]\n",
    "serving_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.dumps(\n",
    "    {\n",
    "        \"signature_name\": \"serving_default\",\n",
    "        \"instances\": [\n",
    "            \n",
    "            {k: v.numpy().tolist()[0] for k, v in batch.items()},\n",
    "            {k: v.numpy().tolist()[0] for k, v in batch.items()}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "model_name = \"creditcard\"\n",
    "url_sig = f\"http://localhost:8501/v1/models/{model_name}:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[-9.40117264], [-9.40117264]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post(url_sig, data=data, headers=headers)\n",
    "print(json.loads(json_response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.dumps(\n",
    "    {\n",
    "        \"signature_name\": \"serving_default\",\n",
    "        \"inputs\": {k: v.numpy().tolist() for k, v in batch.items()},\n",
    "    }\n",
    ")\n",
    "model_name = \"creditcard\"\n",
    "url_sig = f\"http://localhost:8501/v1/models/{model_name}:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': [[-9.40117264], [-8.42035294], [-8.5709095], [-9.22763252], [-7.27975225], [-8.96863], [-9.03657722], [-8.61154556], [-8.86451912], [-6.9543705], [-7.79198074], [-9.9882679], [-9.60381699], [-11.0595407], [-11.539463], [-10.3595982]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post(url_sig, data=data, headers=headers)\n",
    "print(json.loads(json_response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (476313318.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[36], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run  -p 8501:8501 --name creditcard --xla_cpu_compilation_enabled --mount type=bind,source=/Users/stefruinard/Documents/personal/projects/202312_probabilistic_deep_learning/augmented_deep_learning/models/saved_model/creditcard,target=/models/creditcard -e MODEL_NAME=creditcard -t emacski/tensorflow-serving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"flax-recommender-system\"\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "from flax.training.train_state import TrainState\n",
    "from orbax.export import ExportManager, JaxModule, ServingConfig\n",
    "\n",
    "\n",
    "\n",
    "# p = ip.Preprocessor()\n",
    "\n",
    "def save(state, preprocessing_fn, output_dir, etr=None):\n",
    "    # Construct a JaxModule where JAX->TF conversion happens.\n",
    "    jax_module = JaxModule(state.params, state.apply_fn)\n",
    "    # Export the JaxModule along with one or more serving configs.\n",
    "    export_mgr = ExportManager(\n",
    "        jax_module,\n",
    "        [\n",
    "            ServingConfig(\n",
    "                \"serving_default\",\n",
    "                tf_preprocessor=preprocessing_fn,\n",
    "                # tf_postprocessor=exampe1_postprocess\n",
    "                extra_trackable_resources=etr,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    export_mgr.save(output_dir)\n",
    "\n",
    "\n",
    "def load(output_dir):\n",
    "    loaded_model = tf.saved_model.load(output_dir)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def inference(loaded_model, inputs):\n",
    "    loaded_model_outputs = loaded_model(inputs)\n",
    "    return loaded_model_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(state, p.serving_fn, \"./models/flax_recommender_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(\"./models/flax_recommender_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\"inputs\":{\n",
    "            \"V1\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V2\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V3\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V4\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V5\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V6\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V7\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V8\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V9\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V10\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V11\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V12\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V13\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V14\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V15\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V16\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V17\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V18\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V19\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V20\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V21\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V22\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V23\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V24\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V25\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V26\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V27\": tf.constant([1], dtype=tf.float32),\n",
    "            \"V28\": tf.constant([1], dtype=tf.float32),\n",
    "            \"Amount\": tf.constant([1], dtype=tf.float32),\n",
    "        }}\n",
    "\n",
    "\n",
    "# create batch of examples\n",
    "batch_size = 64\n",
    "n_features = 29\n",
    "\n",
    "batch_inputs = {\n",
    "    \"V1\": tf.random.normal((batch_size, )),\n",
    "    \"V2\": tf.random.normal((batch_size, )),\n",
    "    \"V3\": tf.random.normal((batch_size, )),\n",
    "    \"V4\": tf.random.normal((batch_size, )),\n",
    "    \"V5\": tf.random.normal((batch_size, )),\n",
    "    \"V6\": tf.random.normal((batch_size, )),\n",
    "    \"V7\": tf.random.normal((batch_size, )),\n",
    "    \"V8\": tf.random.normal((batch_size, )),\n",
    "    \"V9\": tf.random.normal((batch_size, )),\n",
    "    \"V10\": tf.random.normal((batch_size,)),\n",
    "    \"V11\": tf.random.normal((batch_size,)),\n",
    "    \"V12\": tf.random.normal((batch_size,)),\n",
    "    \"V13\": tf.random.normal((batch_size,)),\n",
    "    \"V14\": tf.random.normal((batch_size,)),\n",
    "    \"V15\": tf.random.normal((batch_size,)),\n",
    "    \"V16\": tf.random.normal((batch_size,)),\n",
    "    \"V17\": tf.random.normal((batch_size,)),\n",
    "    \"V18\": tf.random.normal((batch_size,)),\n",
    "    \"V19\": tf.random.normal((batch_size,)),\n",
    "    \"V20\": tf.random.normal((batch_size,)),\n",
    "    \"V21\": tf.random.normal((batch_size,)),\n",
    "    \"V22\": tf.random.normal((batch_size,)),\n",
    "    \"V23\": tf.random.normal((batch_size,)),\n",
    "    \"V24\": tf.random.normal((batch_size,)),\n",
    "    \"V25\": tf.random.normal((batch_size,)),\n",
    "    \"V26\": tf.random.normal((batch_size,)),\n",
    "    \"V27\": tf.random.normal((batch_size,)),\n",
    "    \"V28\": tf.random.normal((batch_size,)),\n",
    "    # amount is random int\n",
    "    \"Amount\": tf.random.uniform((batch_size,), minval=0, maxval=1000, dtype=tf.float32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.signatures[\"serving_default\"](**example[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.range(10).batch(2).take(3).repeat(2).as_numpy_iterator()\n",
    "for _ in range(2):\n",
    "    \n",
    "    for x in ds:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The Orbax Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "r\"\"\"Export a MNIST JAX model.\n",
    "\n",
    "python flax_mnist_main.py --output_dir=<OUTPUT_DIR>\n",
    "\"\"\"\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from orbax.export import ExportManager\n",
    "from orbax.export import JaxModule\n",
    "from orbax.export import ServingConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = None\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_path = f\"./models/saved_model/creditcard/{current_time}\"\n",
    "\n",
    "\n",
    "\n",
    "class JaxMnist(nn.Module):\n",
    "  \"\"\"Mnist model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    x = nn.log_softmax(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def export_mnist() -> None:\n",
    "  \"\"\"Exports a Mnist JAX model.\"\"\"\n",
    "\n",
    "  # Initialize the model.\n",
    "  model = JaxMnist()\n",
    "  params = model.init(jax.random.PRNGKey(123), jnp.ones((1, 28, 28, 1)))\n",
    "\n",
    "  # Wrap the model params and function into a JaxModule.\n",
    "  jax_module = JaxModule(\n",
    "      params,\n",
    "      model.apply,\n",
    "      trainable=False,\n",
    "    jit_compile=False,\n",
    "       jax2tf_kwargs={\n",
    "          \"enable_xla\": False\n",
    "      },\n",
    "      input_polymorphic_shape='(b, ...)' if batch_size is None else None)\n",
    "\n",
    "  # Specify the serving configuration and export the model.\n",
    "  em = ExportManager(jax_module, [\n",
    "      ServingConfig(\n",
    "          'serving_default',\n",
    "          input_signature=[\n",
    "              tf.TensorSpec([batch_size, 28, 28, 1], tf.float32, name='inputs')\n",
    "          ],\n",
    "          tf_postprocessor=lambda x: dict(outputs=x)),\n",
    "  ])\n",
    "  # Save the model.\n",
    "  logging.info('Exporting the model to %s.', model_path)\n",
    "  em.save(model_path)\n",
    "\n",
    "  # Test that the saved model could be loaded and run.\n",
    "  logging.info('Loading the model from %s.', model_path)\n",
    "  loaded = tf.saved_model.load(model_path)\n",
    "  logging.info('Loaded the model from %s.', model_path)\n",
    "\n",
    "  inputs = jnp.ones((batch_size or 1, 28, 28, 1))\n",
    "  savedmodel_output = loaded.signatures['serving_default'](inputs=inputs)\n",
    "  jax_output = model.apply(params, inputs)\n",
    "\n",
    "  logging.info('Savemodel output: %s, JAX output: %s', savedmodel_output,\n",
    "               jax_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"http://localhost:8501/v1/models/creditcard:predict\"\n",
    "image = jnp.ones((1, 28, 28, 1))\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": image.tolist()})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(url, data=data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -t --rm -p 8501:8501 --mount type=bind,source=/tmp/model_name/,target=/models/model_name/ -e MODEL_NAME=model_name emacski/tensorflow-serving:latest-linux_arm64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
